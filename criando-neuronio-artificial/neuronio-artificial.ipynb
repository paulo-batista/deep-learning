{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Perceptron (Neurônio Artificial) de Frank Rosenblatt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurônio\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "O neurônio pode ser considerado a unidade básica da estrutura do cérebro e do sistema nervoso. A membrana exterior de um neurônio toma a forma de vários ramos extensos chamados dendritos, que recebem sinais elétricos de outros neurônios, e de uma estrutura a que se chama um axônio que envia sinais elétricos a outros neurônios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![title](images/esquemaNeuronio.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "O perceptron é um tipo de neurônio artificial inventado em 1957 no Cornell Aeronautical Laboratory por Frank Rosenblatt. Onde temos as entradas (dentritos) associados a pesos sinápticos (Weights), a área de processamento composta pelo Combinador Linear (Basicamente consolida os valores de entradas associadas ao seus respectivos pesos Soma(X * W) o resultado então passa por uma função de ativação, gerando assim uma saída (axônio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de ativação sigmoid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Outro conceito importante é a função de ativação, por quê ativação ? O neurônio possui dois estados, ligado (Ativo) ou desligado (Desativado). Dependendo dos valores de entrada, ele pode alternar entre seus estados. E a função que determina o ponto de ativação é chamada, função Sigmoid.\n",
    "\n",
    "A função de ativação sigmoid é comumente utilizada por redes neurais com propagação positiva (Feedforward) que precisam ter como saída apenas números positivos, em redes neurais multicamadas e em outras redes com sinais contínuos.\n",
    "\n",
    "Apesar de seu grande uso, a função de ativação tangente hiperbólica é geralmente uma escolha mais adequada.\n",
    "\n",
    "Equação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A função de Ativação é apresentada pelos gráficos: (a) função degrau, (b) função linear, (c) função logística e (d) FUNÇÃO SIGMOID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/grafico.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron em Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para nos auxiliar com operações de álgebra linear .. importamos a biblioteca nympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "O Neurônio artificial será criado pela classe NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    # Inicialização da classe RedeNeural\n",
    "    def __init__(self):\n",
    "        #Seed 1, significa vamos gerar números aleatórios, porém se executarmos 10, 20 ou 100 vezes\n",
    "        # o programa manterá sempre a mesma sequência de valores. \n",
    "        np.random.seed(1)\n",
    "        # A tupla (3,1) indica que vamos construir um neurônio com 3 dentritos ((Entradas)\n",
    "        # e seu axiônico (1 Saída)\n",
    "        self.synaptic_weights = 2 * np.random.random((3,1))-1\n",
    "    \n",
    "    \n",
    "    #Função de Ativação\n",
    "    #A função sigmoid, como vimos descreve a curva S, onde passamos a soma\n",
    "    #dos pesos das sinápses das entradas para esta função para normalização\n",
    "    #entre 0 e 1.\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #Para indicar o quanto podemos confirmar nos pesos das sinapses, devivamos\n",
    "    #função sigmoid\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    #Treinamos um neurônio através de um processo de tentativa e erro\n",
    "    #Ajustando os pesos da sinápses para cada nova análise.\n",
    "    #Em Machine Learning quando colocamos o algoritmo em treinamento\n",
    "    #significa que vamos executá-lo várias vezes em busca da redução \n",
    "    #da margem de erro, ou seja, otimizando os resultados. \n",
    "    \n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in xrange(number_of_training_iterations):\n",
    "            # Passamos os valores de treinamento para o pensamento (processamento) do neurônio\n",
    "            output = self.think(training_set_inputs)\n",
    "           \n",
    "            # Cálculo do erro (A diferença entre resultado desejado e a saída calculada)\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # Multiplicar o erro pela entrada e novamente pelo gradiente da curva Sigmoid.\n",
    "            # Isto significa que pesos se ajustem para obter o resultado cada vez mais otimizado.\n",
    "            # Isto significa entradas, com valores zero, não alterama valores dos pesos.\n",
    "            adjustment = np.dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Ajustando os pesos das sinapses.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # Processamento do pensamento.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        # Passar as entradas através da unidade de neurônio (Perceptron).\n",
    "        return self.__sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do neurônio e processando seu pensamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo randômico dos pesos das sinapses: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Nova peso da sinapses após processo de treinamento: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Considerando os novos valores do novo cenário [1, 0, 0], temos: \n",
      "[ 0.99993704]\n"
     ]
    }
   ],
   "source": [
    "# Instanciando o neurônio.\n",
    "neural_network = NeuralNetwork()\n",
    "\n",
    "print \"Iniciando o processo randômico dos pesos das sinapses: \"\n",
    "print neural_network.synaptic_weights\n",
    "\n",
    "# Definindo o conjunto de treinamento. Nós temos 4 exemplos, e cada exemplo\n",
    "#consiste em 3 valores de entrada e uma saída.\n",
    "training_set_inputs = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Iniciamos o treinamento do neurônio usando o conjunto de treinamento.\n",
    "# Trainamos o neurônio 10.000 vezes, ajustando seus resultados a cada execução.\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "print \"Nova peso da sinapses após processo de treinamento: \"\n",
    "print neural_network.synaptic_weights\n",
    "\n",
    "# Testando o neurônico para um novo cenário.\n",
    "print \"Considerando os novos valores do novo cenário [1, 0, 0], temos: \"\n",
    "print neural_network.think(np.array([1, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Créditos para Siraj Raval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
